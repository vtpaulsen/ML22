{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss\n",
    "\n",
    "In this notebook we introduce the cross entropy loss function, which ends up being a generalization of the loss function derived for softmax using maximum likelihood.\n",
    "\n",
    "## One-in-K Encoding\n",
    "Assume the problem at hand is a multiclass classification problem with $K$ classes.\n",
    "Recall the one-in-K encoding from the softmax note, which represents a label $y \\in \\{1,\\dots,K\\}$ as the $K$-dimensional vector having a $1$ in the $y$'th entry and $0$ elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Outputs\n",
    "As in the softmax note, we are interested in estimating the probabilities $P(y=i\\mid x)$ for $i=1,\\dots K$.\n",
    "This is a list of length $K$ giving the probability of each class based on the feature vector $x$.\n",
    "\n",
    "Assume that we have a machine learning model with parameters $W$, that outputs $K$ probabilities $p_1(x,W),\\dots,p_K(x,W)$ summing to $1$, where the intention is that $p_i(x,W) = P(y=i \\mid x)$. One example of such a model is in multinomial regression/softmax where $p_i(x,W) = \\textrm{softmax}(x^\\intercal W)_i$ for a $d \\times K$ weight matrix $W$.\n",
    "\n",
    "In softmax, we defined the likelihood of a training example $(x,y)$, where $y \\in \\{0,1\\}^K$ is the one-in-K encoding of the label $\\hat{y} \\in \\{1,\\dots,K\\}$, as $y^\\intercal \\textrm{softmax}(x^\\intercal W) = \\textrm{softmax}(x^\\intercal W)_{\\hat{y}} = p_{\\hat{y}}(x,W)$. For a full training set $(x^1,y^1),\\dots,(x^n,y^n)$ (the index is in the superscript to allow indexing the $i$'th coordinate of the $j$'th vector as $y^j_i$), we used the assumption of independence among samples to argue that the likelihood of seeing all the labels $y^1,\\dots,y^n$ was $\\prod_{i=1}^n p_{\\hat{y}^i}(x^i,W)$. Finally, we took negative log-likelihood, divded by $n$, and arrived at the in-sample error:\n",
    "$$\n",
    "E_{in}(W) = -\\frac{1}{n}\\sum_{i=1}^n \\ln(p_{\\hat{y}^i}(x^i,W)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "The cross entropy loss function is a direct generalization of the above. Again, consider any machine learning model that outputs $K$ probabilities $p_1(x,W),\\dots,p_K(x,W)$ summing to $1$. Also, allow labels to be probability distributions over classes, rather than just a single class. That is, each label $y$ is a $K$-dimensional vector with non-negative entries summing to $1$. The one-in-K encoding is a special case of this where all the probability mass has been placed on a single coordinate. \n",
    "\n",
    "We then define the cross entropy loss of a training set $(x^1,y^1),\\dots,(x^n,y^n)$ as:\n",
    "$$\n",
    "E_{in}(W) = -\\frac{1}{n}\\sum_{i=1}^n \\sum_{k=1}^K y^i_k \\ln(p_k(x^i,W)).\n",
    "$$\n",
    "Notice how this loss function equals the loss function above when the labels $y^i$ have only a single entry $\\hat{y}^i$ that is $1$ and the rest are $0$.\n",
    "\n",
    "The cross entropy loss may be used for all models that are intended to predict probability distributions over classes (also distributions with all mass on a single class), not only for the linear model used in multinomial regression.\n",
    "\n",
    "It may be worth noting that if $p_k(x^i,W)=0$ and $y^i_k > 0$, then the loss is infinite. Fortunately, if the probabilities $p_k(x^i,W)$ are generated via softmax, no output probability will ever be $0$. This is worth keeping in mind if the output probabilities of the model are obtained in other ways than taking softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "In most applications, computing the gradient of the loss function is an integral part of the learning algorithm, e.g. in gradient descent. The cross entropy loss function has the following gradient (derived via the chain rule):\n",
    "$$\n",
    "\\nabla_W E_{in}(W) = -\\frac{1}{n} \\sum_{i=1}^n \\sum_{k=1}^K \\frac{y_k^i}{p_k(x^i,W)} \\nabla_W p_k(x^i,W).\n",
    "$$\n",
    "The derivative $\\nabla_W p_k(x^i,W)$ depends on the model used. Remember here that $\\nabla_W p_k(x^i,W)$ is a $d \\times K$ matrix where entry $(a,b)$ stores $\\frac{\\partial p_k(x^i,W)}{\\partial w_{a,b}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming\n",
    "While not particularly important, we briefly motivate why the loss function has been termed cross entropy loss. Recall from the lecture on decision trees the so-called entropy function. If $p_1,\\dots,p_K$ denotes probabilities of $K$ different outcomes, with $\\sum_i p_i = 1$, then the entropy of the corresponding distribution is\n",
    "$$\n",
    "-\\sum_i p_i \\lg p_i\n",
    "$$\n",
    "The entropy, when the base of the log is $2$, gives the expected number of bits necessary to encode/represent a random element drawn from the probability distribution. In the cross entropy loss, if we focus on a single training example, we have a probability distribution $y$ (the label) and another distribution $p=p_1(x,W),\\dots,p_K(x,W)$ with the intention that $p_i(x,W) = y_i$. This training example contributes $-\\sum_i y_i \\ln(p_i(x,W))$ to the loss. This quantity almost equals the entropy, except that we swapped the distributions. It effectively gives the expected number of bits needed to encode a random element drawn from $y$ using the optimal way to encode a random element drawn from $p$ (except we use $\\ln$ in cross entropy rather than $\\lg_2$). This motivates the name of cross entropy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
