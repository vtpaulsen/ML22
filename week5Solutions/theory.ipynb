{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1: Break Points and Growth Functions \n",
    "\n",
    "-   Is there always a break point for a finite hypothesis set of $n$\n",
    "    hypotheses? If so, can you give a an upper bound? What is the growth\n",
    "    function?\n",
    "\n",
    "-   Does the set of all functions have a break point? What is its growth\n",
    "    function?\n",
    "\n",
    "-   What is the (smallest) break point for the hypothesis set consisting\n",
    "    of circles centered around $(0,0)$? For a given circle the\n",
    "    hypothesis returns $1$ for points inside the circle and $-1$ for\n",
    "    points outside. What is the growth function?\n",
    "\n",
    "-   What if we move to centered balls in the 3-dimensional space\n",
    "    ${{\\mathbb R}}^3$? Or in general $d$-dimensional space\n",
    "    ${{\\mathbb R}}^d$ (hyperspheres)?\n",
    "\n",
    "-  Show that the growth function for a singleton hypothesis class $H = \\{h\\}$ is 1\n",
    "\n",
    "\n",
    "### SOLUTION MATH\n",
    "1. Since we have $n$ hypotheses, the growth function is capped at $n$. Therefore it must have a break point of at most $\\lfloor \\lg n \\rfloor + 1$.\n",
    "\n",
    "2. It does not have a break point. Its growth function is $m(n)=2^n$ since all functions can create all dichotomies.\n",
    "\n",
    "3. The break point is at 2 since for two points of distinct distances, it is impossible to construct the dichotomy having $1$ on the point furthest from the origin and $-1$ on the point closest. If 2 points have the same distance, then any dichotomy with two distinct labels is impossible. The growth function is the same as positive rays in 1d (sort by distances), hence $m(n)=1+n$.\n",
    "\n",
    "4. The same since all that matters is the distance to the origin.\n",
    "\n",
    "5. For a single point, it is impossible to create both dichotomies since $h$ only gives one fixed output on the point.\n",
    "\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2: VC Dimension \n",
    "\n",
    "-   Does VC Dimension depend on the learning algorithm or the actual\n",
    "    data set given?\n",
    "\n",
    "-   Does VC Dimension depend on the probability distribution generating\n",
    "    the data (not the labels)?\n",
    "\n",
    "-   If $\\mathcal{H}_1 \\subseteq \\mathcal{H}_2$ is\n",
    "    $VC(\\mathcal{H}_1) \\leq VC(\\mathcal{H}_2)?$\n",
    "\n",
    "-   Can you give an upper bound on the VC dimension of a finite set of\n",
    "    $M$ hypotheses?\n",
    "\n",
    "-   What is the VC Dimension for the hypothesis set consisting of\n",
    "    circles centered around 0?\n",
    "\n",
    "-   What if we move to balls (3d)? or in general d dimensions\n",
    "    (hyperspheres)?\n",
    "\n",
    "-   What is the maximal VC dimension possible of the intersection of\n",
    "    hypothesis sets $\\mathcal{H}_1,\\dots,\\mathcal{H}_n$ with VC\n",
    "    dimension $v_1,\\dots,v_n$.\n",
    "\n",
    "-   As previous question, instead what is the minimal VC dimension of\n",
    "    the union of the hypothesis sets from the previous question\n",
    "\n",
    "-   Show that the VC dimension of the hypothesis set consisting of axis aligned rectangles in $\\mathbb{R}^2$ is 4,\n",
    "    i.e. find a point set of 4 points you can shatter and argue that any point set of size 5 can not.\n",
    "    \n",
    "\n",
    "### SOLUTION MATH\n",
    "1. No\n",
    "2. No\n",
    "3. yes. If you can shatter a point set using $\\mathcal{H}_1$, you can use the same hypotheses to shatter it using $\\mathcal{H}_2$ since $\\mathcal{H}_1 \\subseteq \\mathcal{H}_2$.\n",
    "4. yes, $\\lfloor log M \\rfloor$. To see this, notice that for VC dimension $d$, one must be able to shatter a set of $d$ points, i.e. $M \\geq 2^d$.\n",
    "5. same as positive rays in 1d, namely 1. sort by distance -1 +1 is impossible.\n",
    "6. The same\n",
    "7. The minimum of the VC dimensions since the intersection is contained in all.\n",
    "8. The largest of the VC dimensions since you always keep at least the largest and all others might be subset of it.\n",
    "9. To prove at least 4, pick points (0,1), (0,-1), (1,0), (-1,0). Not hard to see that all dichotomies can be obtained. To prove less than 5, consider any set of 5 points in the plane. Computing their bounding box $B$. All the 5 points are containing in $B$, and to construct the \"full\" dichotomy, one must use a rectangle containing at least $B$. Now consider the dichotomy that contains all points on the boundary of the bounding box but none of the points inside. That dichotomy is impossible if the box $B$ has a point in its interior. Finally, if all 5 points lie on the boundary of the bounding box, there are two that lie on the same side of the bounding box. Any dichotomy involving all but one of those two is impossible.\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3:  Book Exercise\n",
    "### Exercise 1.11 in the [LFD] Book \n",
    "(Not problem, but exercise inside the text. page 25) <b>Hint:</b> You might want to use Hoeffding's inequality for question (c).\n",
    "\n",
    "\n",
    "### SOLUTION MATH\n",
    "a. No. The training data might be such that it contains most examples with the most unlikely label.\n",
    "\n",
    "b. Yes. It might be the case that $p$ is very small, say 0.1, and yet the sample has only points with label +1.\n",
    "\n",
    "c. It equals the probability that $B(n,p)>n/2$ where $n=25$. We can e.g. use Hoeffding to get a bound on this. Let $X \\sim B(n,p)/n$. We see that $\\Pr[|X-p|>1/2-p] \\leq 2\\exp(-2(1/2-p)^2 n) = 2\\exp(-2 \\cdot 0.4^2 \\cdot 25) < 0.00068$. Hence $S$ produces a better hypothesis with probability at least $1-0.00068$.\n",
    "\n",
    "d. No as $S$ outputs the maximum likelihood estimate.\n",
    "\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4: Regularization with Weight decay\n",
    "If we use weight decay regularization ($\\lambda||w||^2)$  for some real number $\\lambda$ in Linear Regression, what \n",
    "happens to the optimal weight vector if we let $\\lambda \\rightarrow \\infty$? (cost is $\\frac{1}{n} \\|Xw - y\\|^2 + \\lambda \\|w\\|^2$)\n",
    "\n",
    "## SOLUTION MATH\n",
    "\n",
    "If we let $\\lambda \\rightarrow \\infty$, then the best choice of $w$ is $0$ as the cost of this solution tends to $\\frac{1}{n}\\|y\\|^2$ whereas the cost of all other $w'$ tend to infinity.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5: Grid Search For Regularization and Validation - Sklearn\n",
    "In this exercise we will optimize a [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) using regularization and validation.\n",
    "You must use the grid search module [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) from sklearn.\n",
    "\n",
    "In the cell below we have shown an example of how to use the grid search module to test two different values for max_depth for a decision tree for wine classification\n",
    "\n",
    "Your job is to find good hyperparameters for decision trees for breast cancer detection.\n",
    "\n",
    "### Task 1:\n",
    "For the breast cancer data set, find the best (or very good) combination of max_depth and min_samples_split (cell two below)\n",
    "\n",
    "The **max_depth** parameter controls the max depth of a tree and the deeper the tree the more complex the model.\n",
    "\n",
    "The **min_samples_split** controls how many elements the algorithm that constructs the tree is allowed to try and split.\n",
    "So if a subtree contains less than min_leaf_size elements, it many not be split into a larger subtree by the algorithm.\n",
    "\n",
    "\n",
    "### Task 2:\n",
    "- How long time does it take to use grid search validation for $k$ hyperparameters where we test each parameter for $d$ values, and the training algorithm uses $f(n)$ time to train on $n$ data points when we split the data into 5 parts.\n",
    "\n",
    "\n",
    "### SOLUTION MATH\n",
    "\n",
    "For task 2: There are $d^k$ combinations of hyperparameters to test. For each, we train $5$ times on $(4/5)n$ points. So the total running time is $d^k 5 f((4/5)n) = O(d^k f((4/5)n))$. For non-decreasing $f$, this is bounded by $O(d^k f(n))$.\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine, load_breast_cancer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "def show_result(clf):\n",
    "    df = pd.DataFrame(clf.cv_results_)\n",
    "    df = df.sort_values('mean_test_score', ascending=False)\n",
    "    display(df)\n",
    "    print('best parameter found', clf.best_params_)\n",
    "    \n",
    "w_data = load_wine()\n",
    "wine_data = w_data.data\n",
    "wine_labels = w_data.target\n",
    "\n",
    "# grid search validation\n",
    "reg_parameters = {'max_depth': [1, 30]}  # dict with all parameters we need to test\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), reg_parameters, cv=3, return_train_score=True)\n",
    "clf.fit(wine_data, wine_labels)\n",
    "# code for showing the result\n",
    "bt = show_result(clf)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = load_breast_cancer()\n",
    "c_data = cancer_data.data\n",
    "c_labels = cancer_data.target\n",
    "\n",
    "\n",
    "def decisiontree_model_selection(train_data, labels):\n",
    "    clf = None\n",
    "    ### YOUR CODE HERE\n",
    "    reg_parameters = {'max_depth': [5, 10, 15],\n",
    "                      'min_samples_split': [2, 5, 10]}                \n",
    "    clf = GridSearchCV(DecisionTreeClassifier(), reg_parameters, cv=3, return_train_score=True)\n",
    "    clf.fit(train_data, labels)\n",
    "    ### END CODE\n",
    "    return clf\n",
    "###\n",
    "clf = decisiontree_model_selection(c_data, c_labels)\n",
    "bt = show_result(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 6: VC Dimension of Hyperplanes (Book Exercise 2.4 p. 52)\n",
    "Consider the input space $\\mathcal{X} = \\{1\\} \\times \\mathbb{R}^d$ (with the first coordinate being the constant 1). Show that the VC dimension of the hypothesis space $\\mathcal{H} = \\{\\textrm{sign}(w^\\intercal x) \\mid w\\in \\mathbb{R}^{d+1} \\}$ corresponding to the perceptron is $d+1$.\n",
    "\n",
    "We need to show \n",
    "1. That there exists a data set of size d+1 that can be shattered by hyperplanes\n",
    "2. That no data set of size d+2 can be shattered by hyperplanes\n",
    "\n",
    "We will give a few more hints than the book does.\n",
    "### Shattering d+1 points\n",
    "As the book hints you must create an \"easy\" data set that you store in matrix $X$. \n",
    "\n",
    "**Hint:** We suggest you consider as a data matrix, the $(d+1) \\times (d+1)$ matrix $X$ whose first column is all-1s (required since $\\mathcal{X} = \\{1\\} \\times \\mathbb{R}^d$) and where the lower $d \\times d$ corner of the matrix is the $d \\times d$ identity matrix.\n",
    "\n",
    "Show that you can construct any dichotomy $y \\in \\{-1,+1\\}^{d+1}$ using some $h \\in \\mathcal{H}$ and the data matrix $X$ defined above. That is, you have to show that for any $y \\in \\{-1,+1\\}^{d+1}$, you can find some hypothesis $w$ such that for all $i$, we have $\\textrm{sign}(w^\\intercal x_i)=y_i$ where $x_i$ is the $i$'th row of $X$.\n",
    "\n",
    "### SOLUTION MATH\n",
    "Given a dichotomy $y \\in \\{-1,+1\\}^{d+1}$, we pick the hypothesis $w$ such that $w_1 = 0.1y_1$ and $w_i = y_i$ for $i>1$. Observe that for the first row of $X$ (data item $x_1$), we have $w^\\intercal x_1 = w_1 = 0.1 y_1$, hence $\\textrm{sign}(w^\\intercal x_1) = y_1$. For $i>1$, notice that $w^\\intercal x_i = w_1 + w_i = 0.1y_1 + y_i$. Since $y_1$ is multiplied by the factor $0.1$, we still have $\\textrm{sign}(w^\\intercal x_i) = y_i$. \n",
    "\n",
    "Here is a another solution. Observe that the matrix $X$ is invertable. Now merely set $w=X^{-1}y$ for an arbitrary dichotomy encoded by $y$. Hence $Xw = XX^{-1}y= y$. \n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "\n",
    "### No Shattering of d+2 points.\n",
    "Must show that for any d+2 points, there is a  dichotomy hyperplanes can not capture.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Consider an arbitrary set of d+2 points $x_1,\\dots, x_{d+2}$ of dimension (d+1) and think of them as vectors in $\\{1\\} \\times \\mathbb{R}^d \\subset \\mathbb{R}^{d+1}$.\n",
    "Since we have more vectors than dimensions the vectors must be linearly dependent.\n",
    "\n",
    "i.e. there is a $j$ such that:\n",
    "$$\n",
    "x_j = \\sum_{i\\neq j} a_i x_i\n",
    "$$\n",
    "Since $x_j$ is determined by the other data points then so is $w^\\intercal x_j$ for any $w$. This means the classification on point $x_j$ is dictated by the classification of the other data points and thus cannot freely be chosen.\n",
    "i.e.\n",
    "$$\n",
    "w^\\intercal x_j = w^\\intercal \\sum_{i\\neq j} a_i x_i =\\sum_{i\\neq j} a_i w^\\intercal x_i\n",
    "$$\n",
    "Define an impossible dichotomy as follows. \n",
    "$$\n",
    "y_i = \\textrm{sign}(a_i), \\quad i\\neq j, \\quad y_j = -1\n",
    "$$\n",
    "Show this dichotomy is impossible!\n",
    "\n",
    "### SOLUTION MATH\n",
    "We have \n",
    "$$\n",
    "w^\\intercal x_j =\\sum_{i\\neq j} a_i w^\\intercal x_i\n",
    "$$\n",
    "Furthermore, we must have $\\textrm{sign}(a_i) = y_i$ for $i \\neq j$ and $\\textrm{sign}(w^\\intercal x_i) = y_i$ for $i \\neq j$. This means that $a_i w^\\intercal x_i$ is positive for $i \\neq j$. Hence $w^\\intercal x_j > 0$ and we cannot have $\\textrm{sign}(w^\\intercal x_j) = -1$.\n",
    "### END SOLUTION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS Exercise If Time 7: Book Problem 2.18 In short\n",
    "Define\n",
    "$$\n",
    "\\mathcal{H}= \\{h_\\alpha \\mid h_\\alpha(x) = (-1)^{\\lfloor \\alpha\n",
    "          x\\rfloor}, \\alpha \\in {{\\mathbb R}}\\}\n",
    "$$ \n",
    "\n",
    "Show that the VC dimension of ${{\\mathcal H}}$ is infinite (even though there is only one parameter!)\n",
    "\n",
    "Hint: Use the points set\n",
    "$x_1=10,x_2=100,\\dots,x_i = 10^i,\\dots,x_N=10^N$ and show how to implement any dichotomy $y_1,\\dots,y_N \\in \\{-1, +1\\}^N$ (find $\\alpha$ that works).\n",
    "You can safely assume $\\alpha >0$.\n",
    "\n",
    "\n",
    "### SOLUTION MATH\n",
    "Map $y$ to $\\alpha$ as follows. define \n",
    "$$\n",
    "f(y) = \\begin{cases}\n",
    "1 \\textrm{ if } y = -1\\\\\n",
    "2 \\textrm{ if } y = +1\n",
    "\\end{cases}\n",
    "$$\n",
    "set $\\alpha$ to the number defined by concatenating the mapped digits $0.f(y_1)f(y_2)\\dots f(y_N)$\n",
    "This means that for $y = [-1, +1, -1]$, $\\alpha = 0.121$.\n",
    "\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
