{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Exercises week 3\n",
    "**Like last week, it is very imporant that you try to solve every exercise. \n",
    "If you do not solve the exercise, focus on understanding the question, and try to figure out what it is you do not understand.**\n",
    "\n",
    "\n",
    "The TA's will be very happy to answer questions during the TA session or on the board.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feasibility of Learning\n",
    "\n",
    "\n",
    "## Exercise 1.12 from [LFD]:\n",
    "A friend comes to you with a learning problem. She says the target function is <i>completely</i> unknown, but she has 4000 data points. She is willing to pay you to solve her problem and produce for her a $g$ which approximates $f$. What is the best that you can promise her among the following:\n",
    "\n",
    "<b>a</b> After learning you will provide her with a $g$ that you will guarantee approximates $f$ well out of sample.\n",
    "\n",
    "<b>b</b> After learning you will provide her with a $g$, and with high probability the $g$ which you produce will approximate $f$ well out of sample.\n",
    "\n",
    "<b>c</b> One of two things will happen.\n",
    "<b>(i)</b> You will produce a hypothesis $g$;\n",
    "<b>(ii)</b> You will declare that you failed.\n",
    "If you do return a hypothesis $g$, then with high probability the $g$ which you produce will approximate $f$ well out of sample.\n",
    "\n",
    "### solution math\n",
    "Option a. is not possible as $f$ can always look completely different from your choice $g$ outside the samples when $f$ is completely unknown.\n",
    "\n",
    "Option b. is also not possible. For instance, it could be the case that the hypothesis set you decide on does not even contain a hypothesis that looks like $f$. \n",
    "\n",
    "Option c. is possible. Once you trained, you can test the in-sample-error. If this is high, report failed. If it is low, then you can either use Hoeffding's inequality or a test set to argue whether it is likely that $g$ and $f$ are also similar out of sample. If you did not have enough data compared to the number of hypotheses you search through, you can jut report failed. Otherwise you can return $g$.\n",
    "\n",
    "### end solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. In-Sample and Out-of-Sample Error\n",
    "\n",
    "Consider an input domain $X=\\{x_1,x_2,x_3,x_4\\}$. Let the unknown target function $f$ assign labels $1$ to all $4$ elements in $X$.\n",
    "\n",
    "Consider a hypothesis $h$ such that $h(x_1)=h(x_2)=h(x_3)=1$ and $h(x_4)=-1$.\n",
    "\n",
    "<b>Question 1</b>: What is the worst possible $E_{in}(h)$ over any data set $S$ of $5$ samples of the form $(x,f(x))$ where $x \\in X$?\n",
    "\n",
    "<b>Question 2</b>: What is the best possible $E_{in}(h)$ over any data set $S$ of $5$ samples of the form $(x,f(x))$ where $x \\in X$?\n",
    "\n",
    "<b>Question 3</b>: Consider an unknown distribution $D$, such that $D(x_1)=D(x_2)=D(x_3)=1/6$ and $D(x_4)=1/2$ (here $D(x_i)$ denotes the probability of seeing the sample $x_i$). What is $E_{out}(h)$ under this distribution? What is the worst possible $E_{in}(h)$ we could get if we sample $5$ elements from $D$? What is the best $E_{in}(h)$?\n",
    "\n",
    "<b>Question 4</b>: Give an example of a distribution $D$ over $X$ such that $E_{out}(h)=1$.\n",
    "\n",
    "<b>Question 5</b>: Give an example of a distribution $D$ over $X$ such that $E_{out}(h)=0$.\n",
    "\n",
    "\n",
    "### Solution math\n",
    "Q1: We can get $E_{in}(h)=1$ if we see $5$ copies of $x_4$ in our sample $S$.\n",
    "\n",
    "Q2: We can get $E_{in}(h)=0$ if we see $5$ copies of $x_1$ in our sample.\n",
    "\n",
    "Q3: We have $E_{out}(h) = \\Pr_{x \\sim D}[h(x)\\neq f(x)] = \\Pr_{x \\sim D}[x = x_4] = 1/2$. We could be unlucky and see $5$ copies of $x_4$ and have $E_{in}(h)=1$. We could also be lucky and see $5$ elements among $x_1,x_2,x_3$ and get $E_{in}(h)=0$.\n",
    "\n",
    "Q4: The distribution $D$ has $D(x_4)=1$.\n",
    "\n",
    "Q5: One such distribution $D$ has $D(x_1)=1$.\n",
    "\n",
    "### End Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generalization\n",
    "\n",
    "\n",
    "## Questions:\n",
    "The Hoeffding bound gives us the following guarantee:\n",
    "$$\n",
    "\\Pr[|E_{in}-E_{out}| > \\varepsilon] \\leq 2Me^{-2\\varepsilon^2 n},\n",
    "$$\n",
    "where the probability is over the random choice of the sample.\n",
    "\n",
    "<b>Question 1: </b> \n",
    "Does the Hoeffding bound give any meaningful bounds on $E_{in}$ and $E_{out}$ for the perceptron learning model?  \n",
    "\n",
    "<b>Question 2: </b> \n",
    "Assume you use a hypothesis set $H$ with $|H|=M=10^6$. How many samples $n$ do you need to guarantee that $|E_{in} - E_{out}| < 0.1$ with probability at least $0.95$? \n",
    "\n",
    "<b>Question 3: </b> \n",
    "Assume you use a hypothesis set $H$ with $|H|=M=10^6$ and you have $n=10^4$ samples. How small can you guarantee that $|E_{in} - E_{out}|$ is with probability at least $0.95$? \n",
    "\n",
    "<b>Question 4: </b> \n",
    "Assume you have $n=10^4$ samples and would like to guarantee that $|E_{in} - E_{out}|<0.2$ with probability at least $0.95$. How large a hypothesis set can you use?\n",
    "\n",
    "\n",
    "### solution math\n",
    "Q1. No since $M$ is infinite\n",
    "\n",
    "Q2. Need $2 \\cdot 10^6 \\cdot e^{-n/50} < 0.05$. Satisfied if $-n/50 < \\ln(0.05/(2 \\cdot 10^6))$, which is satisfied for $n > 50 \\ln((2 \\cdot 10^6)/0.05) = 875.2$.\n",
    "\n",
    "Q3. Need $2 \\cdot 10^6 \\cdot e^{-2 \\cdot 10^4 \\varepsilon^2} < 0.05$. Satisfied if $\\varepsilon > \\sqrt{\\ln(2 \\cdot 10^6/0.05)/(\\cdot 2 \\cdot 10^4)} = 0.0296$.\n",
    "\n",
    "Q4. Need $2 \\cdot M \\cdot e^{-(2/25) \\cdot 10^4} < 0.05$. Satisfied if $M < 0.05 \\cdot e^{(2/25) \\cdot 10^4}/2 = e^{800}/40$.\n",
    "### end solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Noisy Targets\n",
    "In the lectures, we extended the supervised learning setup to a case where the labels are noisy. Formally, for any feature vector $x$, there is a distribution $P(y \\mid x)$ on the label $y$.\n",
    "\n",
    "Assume the data distribution $D$ gives a uniform random feature vector among a fixed set of three vectors $x_1,x_2$ and $x_3$.\n",
    "\n",
    "Assume that $Pr(y=1 \\mid x_1) = 2/3$ (and thus $Pr(y=-1 \\mid x_1) = 1/3$). Also, assume $Pr(y=1 \\mid x_2) = 1/2$ and $Pr(y=1 \\mid x_3) = 1/4$.\n",
    "\n",
    "<b>Question</b>: What is the best possible out-of-sample error that any hypothesis $h : \\{x_1,x_2,x_3\\} \\to \\{-1,1\\}$ can achieve? And what are the predictions made by that hypothesis $h$?\n",
    "\n",
    "<b>Question (*A bit hard, so maybe skip)</b>: Does it help to allow a <i>randomized</i> hypothesis? A randomized hypothesis $h$ is one such that on any given $x$, we randomly output $1$ with probability $p_{h,x}$ and $-1$ with probability $1-p_{h,x}$. Here the probability $p_{h,x}$ thus depends on both $h$ and $x$. \n",
    "\n",
    "Hint: Try to look at an example where $Pr(y=1 \\mid x)=p$ for a $p \\geq 1/2$ and assume a randomized hypothesis $h$ predicts $1$ with probability $p_{h,x}$. Then determine the best choice of $p_{h,x}$ to minimize the chance of mispredicting the label.\n",
    "\n",
    "\n",
    "### solution math\n",
    "For the first question, the best you can do if you need to predict the label of $x_1$ is to return $1$. For $x_2$, it does not matter, and for $x_3$, you should predict $-1$. Since $D$ is uniform on the three elements $x_1,x_2,x_3$, the out-of-sample error is $(1/3)(1/3) + (1/3)(1/2) + (1/3)(1/4) = 13/36$.\n",
    "\n",
    "For question b., consider just a single element $x$ such that $Pr(y=1 \\mid x)=p$. Assume wlog. $p \\geq 1/2$. The best deterministic prediction for the label is clearly $1$. For a randomized hypothesis $h$, let $\\sigma$ denote the probability that $h$ outputs $1$ on $x$. We have that $h(x)$ is independent of $y$ and thus $Pr(h(x)=y) = p\\sigma + (1-p)(1-\\sigma)$. To maximize this expression, we could take the derivative wrt. $\\sigma$ and obtain $p - (1-p) = -1+2p$. Since $p \\geq 1/2$, the derivative is at least $0$, i.e. the expression $p\\sigma + (1-p)(1-\\sigma)$ is increasing as a function of $\\sigma$. Since $\\sigma$ lies between $0$ and $1$, it is maximized for $\\sigma=1$. That is, the deterministic choice is best.\n",
    "\n",
    "### end solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Linear Regression and the missing inverse\n",
    "In linear regression, given data matrix $X$ and labels vector $y$ the optimal weight vector $w$ (minimizing $\\|Xw-y\\|_2^2$), is found simply by computing the matrix product\n",
    "$$\n",
    "(X^\\intercal X)^{-1}X^\\intercal y\n",
    "$$\n",
    "That only makes sense if $(X^\\intercal X)$ is in fact invertible.\n",
    "\n",
    "In the lectures I suggested that if indeed $(X^\\intercal X)$ is not invertible then we should remove linear dependent columns from $X$.\n",
    "In this exercise you must argue that this is a good idea.\n",
    "\n",
    "To do this, you must prove/argue the two following things\n",
    "* If $(X^\\intercal X)$ is not invertible then $X$ contains linearly dependent columns\n",
    "* Removing linear dependent columns from X does not change the cost of an optimal solution $w$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HINT 1: you can use a well known linear algebra fact that rank(X) = rank($X^\\intercal X$) = rank($X X^\\intercal$)\n",
    "\n",
    "HINT 2: $Xw$ is in the column space of $X$\n",
    "\n",
    "### solution math\n",
    "Let $X$ be $n \\times d$. $(X^\\intercal X)$ a $d \\times d$ matrix. If it is not invertible, then its rank is less than $d$. Since rank($(X^\\intercal X)$)=rank($X$) and rank($X$) is equal to the rank of the column space of $X$, it follows that the column space of $X$ must have rank less than $d$. Hence there must exist a set of linearly dependent columns. \n",
    "\n",
    "To see that the cost of the optimal solution does not change when removing columns, let $X'$ be the matrix obtained by removing one column of $X$ which is linearly dependent of some subset of the remaining columns in $X'$. Since $Xw$ is in the span of the columns of $X$ for any vector $w$, it must be the case that $Xw$ is also in the span of the columns of $X'$ for every $w$. Hence for any vector $w$, there is a vector $w'$ such that $\\|X'w'-y\\|_2^2 = \\|Xw-y\\|_2^2$. Thus we only need to argue that there cannot exists a vector $w'$ with $\\|X'w'-y\\|_2^2 < \\|Xw-y\\|_2^2$ for all $w$. But this is trivially true as we can pad $w'$ with zeroes and obtain $w''$ such that $Xw''=X'w'$.\n",
    "\n",
    "### end solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Matrix of Derivatives\n",
    "\n",
    "In Linear Regression we define the in sample error as (ignoring the normalizing factor 1/n)\n",
    "\n",
    "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 $$ \n",
    "\n",
    "Let $X$ be the the data matrix of shape $n \\times d$ with data point $x_i$ as the $i$'th row. Let $y$ be the label vector of shape $n \\times 1$ with label $y_i$ as the $i$'th entry. Let $w$ be the weight vector of shape $d \\times 1$.  \n",
    "\n",
    "$$X=\\begin{pmatrix}\n",
    "- & x_1^T & - \\\\\n",
    "- & \\vdots & - \\\\\n",
    "- & x_n^T & - \\\\\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times d}\\quad\\quad\\quad\n",
    "y=\\begin{pmatrix}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times 1}$$\n",
    "\n",
    "The in-sample error rate $E_{in}$ is then equal to \n",
    "\n",
    "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 = \\|Xw-y\\|^2 = (Xw-y)^\\intercal (Xw-y)$$\n",
    "\n",
    "\n",
    "In the lecture we proved that for Linear Regression the optimal weight vector $w_\\textrm{lin}$ for minimizing $E_\\textrm{in}$ was $w_\\textrm{lin}=(X^\\intercal X)^{-1} X^\\intercal y$. \n",
    "\n",
    "To do this we used facts about the derivatives. \n",
    "* If we have a function $f(z): \\mathbb{R}^{a} \\rightarrow \\mathbb{R}^b$ such that $f(z) = [f_1(z),\\dots, f_b(z)]$ then the matrix of derivatives $\\frac{\\partial f}{\\partial z}$ is of size $b\\times a$ where\n",
    "$$ \\left[\\frac{\\partial f}{\\partial z} \\right]_{i,j} = \\frac{\\partial f_i}{\\partial z_j} $$\n",
    "\n",
    "For example: Let $f(x): R^2 \\rightarrow R^3$ be the function $f([x_1, x_2]) = [x_1, x_2, x_1 \\cdot x_2]$ then \n",
    "the matrix of derivatives has shape $3 \\times 2$ and looks like $ \\frac{\\partial f}{\\partial x} =\n",
    " \\begin{bmatrix}\n",
    "  1 & 0 \\\\\n",
    "  0 & 1  \\\\\n",
    "  x_2  & x_1 \\\\\n",
    " \\end{bmatrix} $\n",
    "\n",
    "In our proof we used the following identities about the matrix of derivatives\n",
    "\n",
    "* $f: R^d \\rightarrow R^n, f(z) = Xz-y$, the matrix of derivatives $\\frac{\\partial f}{\\partial z}$ is $X$\n",
    "* $g: R^d \\rightarrow R, g(z) = z^\\intercal z$, the matrix of derivatives $\\frac{\\partial g}{\\partial z}$ is $2z^\\intercal$\n",
    "\n",
    "\n",
    "## Your job is to prove the two identities. \n",
    "\n",
    "* Let $f(z) = Xz - y$, Where $X$ is a $n \\times d$ matrix, $y$ is a $n\\times 1$ vector and $z$ is a $d \\times 1$ vector (function from $\\mathbb{R}^d \\rightarrow \\mathbb{R}^n$). \n",
    "Show that the matrix of derivatives of $f$ is X. \n",
    "\n",
    "Hint: You can think of $f = [f_1,\\dots,f_n]$ as n output functions where $f_i(z) = x_{i}^\\intercal z - y_i$ and $x_i$ is the i'th row of $X$ (as a column vector) and $y_i$ is the i'th entry in vector $y$. Start with $\\frac{\\partial f_1}{\\partial z_1}$ to see if a pattern emerges\n",
    "* Let $g(z) = z^\\intercal z$ where z is a vector (the squared norm of $z$). Show that the matrix of derivatives is $2z^\\intercal$\n",
    "\n",
    "### solution math\n",
    "We have\n",
    "$$\n",
    "\\frac{\\partial f_i}{\\partial z_j} = x_{i,j}\n",
    "$$\n",
    "And thus by definition, we get that the matrix of derivates equals $X$.\n",
    "\n",
    "Next observe that\n",
    "$$\n",
    "\\frac{\\partial g}{\\partial z_j} = \\sum_i \\frac{\\partial z_i^2}{\\partial z_j} = 2z_j\n",
    "$$\n",
    "and the identity follows.\n",
    "### end solution\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Implementing Linear Regression\n",
    "\n",
    "In this exercise your task is to implement Linear Regression.\n",
    "\n",
    "See **linear_regression.py** for starter code.\n",
    "\n",
    "**You need to complete the LinearRegressor class by completing the following methods**\n",
    "- implement hardcode_bias \n",
    "- implement predict \n",
    "- implement score\n",
    "- implement fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
