{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 7 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1: Convolution and Pooling\n",
    "In this exercise we will see if we can get a better grasp of convolution and pooling.\n",
    "Your job is to implement basic convolution and pooling.\n",
    "\n",
    "For this we need the python package pillow  which you need to install\n",
    "\n",
    "- The Convolution operator that takes a $d \\times d$ weight matrix and a 1 channel image (e.g. gray scale), and applies the convolution between the weight matrix and the image.\n",
    "- The max pooling operator takes an input image and a max pooling size and returns the pooled output.\n",
    "    For simplicity we only consider 2 x 2 max pooling with a stride of 2.\n",
    "- Test your convolution implementation with the $3 \\times 3$ matrix with -1 everywhere except in the middle where it is 8. This is a classic edge detector pattern also seen in the lecture.\n",
    "- Test your pooling implementation by applying a $2 \\times 2$ max pooling to the output of the convolution.\n",
    "\n",
    "We assume that we pad the input to ensure the output of the convolution is the same width and height as the input image.\n",
    "\n",
    "To compare your implementation we have supplied code that applies the convolution and the pooling operator from the neural net package in pytorch.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image #(package name is pillow - i.e. pip3 install pillow )\n",
    "import os\n",
    "filename = 'tiger.bmp'\n",
    "\n",
    "img = Image.open(filename)\n",
    "tiger = np.array(img)\n",
    "print('image shape', tiger.shape)\n",
    "plt.imshow(tiger, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "def conv2d(img, w):\n",
    "    \"\"\" Return the result of applying the convolution defined by w to img - \n",
    "    for simplicity assume that w is square\"\"\"\n",
    "    w_dim = w.shape[0]\n",
    "    pad = w_dim - 2\n",
    "    padded_img = np.pad(img, [pad, pad], 'constant', constant_values=0)    \n",
    "    out = np.zeros(img.shape)\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    return out\n",
    "\n",
    "def max_pool2d(img):\n",
    "    \"\"\" Return the result of applying the 2 x 2 max pooling operator to mig (halve the width and height of image)\"\"\"\n",
    "    out = np.zeros((int(img.shape[0]/2), int(img.shape[1]/2)))\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    return out\n",
    "\n",
    "conv_filter = np.array([[-1., -1., -1.], [-1., 8, -1.], [-1., -1., -1.]])\n",
    "convoluted_tiger = conv2d(tiger, conv_filter)\n",
    "pooled_tiger = max_pool2d(convoluted_tiger)\n",
    "plot_min = convoluted_tiger.mean()-convoluted_tiger.std()\n",
    "plot_max = convoluted_tiger.mean()+convoluted_tiger.std()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 16))\n",
    "axes[0].imshow(convoluted_tiger, cmap='gray', vmin=plot_min, vmax=plot_max)\n",
    "axes[1].imshow(pooled_tiger, cmap='gray', vmin=plot_min, vmax=plot_max)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch_tiger = torch.from_numpy(tiger).view(1, 1, tiger.shape[0], tiger.shape[1]).double()\n",
    "print('original',tiger.min(),tiger.max())\n",
    "print('image shape', torch_tiger.shape)\n",
    "tv = torch.tensor([[-1., -1., -1.], [-1., 8, -1.], [-1., -1., -1.]])\n",
    "tv = tv.view(1, 1, 3, 3).double()\n",
    "torch_convoluted_tiger = F.conv2d(torch_tiger, tv, torch.tensor([0.], dtype=torch.double), 1, 1, 1, 1)\n",
    "numpy_convoluted_tiger = torch_convoluted_tiger.numpy().squeeze()\n",
    "print('convoluted_tiger shape', numpy_convoluted_tiger.shape)\n",
    "print('conv diff norm', np.linalg.norm(numpy_convoluted_tiger - convoluted_tiger))\n",
    "torch_pooled_tiger = F.max_pool2d(torch_convoluted_tiger, kernel_size=(2, 2))\n",
    "numpy_pooled_tiger = torch_pooled_tiger.numpy().squeeze()\n",
    "print('pool diff norm', np.linalg.norm(numpy_pooled_tiger - pooled_tiger))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 16))\n",
    "plot_data_min = numpy_convoluted_tiger.mean()-numpy_convoluted_tiger.std()\n",
    "plot_data_max = numpy_convoluted_tiger.mean()+numpy_convoluted_tiger.std()\n",
    "axes[0].imshow(numpy_convoluted_tiger, cmap='gray', vmin=plot_data_min, vmax=plot_data_max)\n",
    "axes[1].imshow(numpy_pooled_tiger, cmap='gray', vmin=plot_data_min, vmax=plot_data_max)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2:  Bias Variance \n",
    "\n",
    "-   Does the Bias and Variance terms (two numbers) in the Bias Variance\n",
    "    decomposition depend on the learning algorithm?\n",
    "\n",
    "-   What is the Variance (in Bias Variance tradoff) if we have a hypothesis\n",
    "    set of size $1$, namely the constant model $h(x) = 2$? The learning\n",
    "    algorithm always picks this hypothesis no matter the data.\n",
    "\n",
    "-   What is the Variance (in the Bias Variance tradeoff) if the simple\n",
    "    hypothesis from the previous question is replaced by a very\n",
    "    sophisticated hypothesis? (Like $h(x)=101x^3 + 6x^2-67x+6$).\n",
    "\n",
    "-   Assume the target function is a second degree polynomial, and the\n",
    "    input to your algorithm is always eleven distinct (noiseless) points. Your\n",
    "    hypothesis set is the set of all degree 10 polynomials and the\n",
    "    learning algorithm returns the hypothesis with the best fit\n",
    "    (miniming least squared error) given the data. What is the Bias and what\n",
    "    is the Variance of this learning algorithm?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3: Bagging\n",
    "In this exercise, you must implement bagging for regression. As a base learning algorithm, we use regression trees trained on the bootstrap samples.\n",
    "\n",
    "Your task is to implement:\n",
    "\n",
    "    -bootstrap_sample: sample m items with replacement from the training data.\n",
    "\n",
    "    -fit: train a number of regression trees on bootstrap samples of the training data.\n",
    "\n",
    "    -predict: return the average prediction among the regression trees trained on bootstrap samples.\n",
    "\n",
    "    -score: return the mean least squares loss.\n",
    "\n",
    "Your implementation should result in significantly better test error (around half) when using bagging rather than a single regression tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "    getattr(ssl, '_create_unverified_context', None)): \n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    \n",
    "class BaggingRegressor():\n",
    "    \n",
    "    def __init__(self, num_trees, sample_fraction):\n",
    "        self.num_t = num_trees\n",
    "        self.p = sample_fraction\n",
    "        self.trees = []\n",
    "        \n",
    "    def bootstrap_sample(self, X, y, m):\n",
    "        \"\"\" Returns a new data matrix containing m samples with replacement from (X,y)\n",
    "    \n",
    "        Args:\n",
    "           X: np.array (n, d)  features\n",
    "           y: np.array (n, ) targets\n",
    "           m: int, number of bootstrap samples\n",
    "    \n",
    "        returns X_boot, y_boot: np.array shape (m,d), np.array (m, ). Bootstrap samples. \n",
    "        A data matrix and vector of labels, where each example is a uniform sample from (X,y) \n",
    "        with replacement (same element can be sampled multiple times).\n",
    "        \"\"\"\n",
    "        X_boot = None\n",
    "        y_boot = None\n",
    "        ### YOUR CODE HERE\n",
    "        ### END CODE\n",
    "        return X_boot, y_boot\n",
    "    \n",
    "    def fit(self, data, targets):\n",
    "        \"\"\" Use bagging to fit multiple regressions trees to the data\n",
    "    \n",
    "        Args:\n",
    "           data: np.array (n, d)  features\n",
    "           targets: np.array (n, ) targets\n",
    "    \n",
    "        Appends self.num_t regression trees to self.trees, each trained on a bootstrap sample \n",
    "        consisting of m = self.p * n examples\n",
    "        \"\"\"\n",
    "        for i in range(self.num_t):\n",
    "            clf = DecisionTreeRegressor()\n",
    "            m = (int)(self.p * data.shape[0])\n",
    "            ### YOUR CODE HERE\n",
    "            ### END CODE\n",
    "            self.trees.append(clf)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Bagging prediction algorithm\n",
    "\n",
    "        Args\n",
    "            X: np.array, shape n,d\n",
    "        \n",
    "        returns pred: np.array shape n,  model predictions on X. Average of predictions made by \n",
    "        regression trees in self.trees\n",
    "        \"\"\"\n",
    "        pred = None\n",
    "        ### YOUR CODE HERE\n",
    "        ### END CODE\n",
    "        return pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute mean least squares loss of the model\n",
    "\n",
    "        Args\n",
    "            X: np.array, shape n,d\n",
    "            y: np.array, shape n, \n",
    "\n",
    "        returns out: scalar - mean least squares loss.\n",
    "        \"\"\"\n",
    "        out = None\n",
    "        ### YOUR CODE HERE\n",
    "        ### END CODE\n",
    "        return out    \n",
    "    \n",
    "def main():  \n",
    "    #load data\n",
    "    housing = fetch_california_housing()\n",
    "    # split 80/20 train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(housing.data,\n",
    "                                                        housing.target,\n",
    "                                                        test_size=0.2)\n",
    "\n",
    "    baseline_accuracy = np.mean((y_test-np.mean(y_train))**2)\n",
    "    print('Least Squares Cost of learning mean of training data:', baseline_accuracy) \n",
    "    \n",
    "    #Regression tree\n",
    "    clf = DecisionTreeRegressor()\n",
    "    clf.fit(X_train,y_train)\n",
    "    predict = clf.predict(X_test)\n",
    "    regression_tree_accuracy = np.mean((y_test - predict)**2)\n",
    "    print('Least Squares Cost of RegressionTree:',regression_tree_accuracy)\n",
    "    \n",
    "    #Bagging\n",
    "    bag = BaggingRegressor(20, 0.7)\n",
    "    bag.fit(X_train,y_train)\n",
    "    predict = bag.predict(X_test)\n",
    "    bagging_accuracy = np.mean((y_test - predict)**2)\n",
    "    print('Least Squares Cost of Bagging:',bagging_accuracy)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4: Bias Variance Experiment \n",
    "In this exercise you must redo the experiment shown at the lectures.\n",
    "This exercise takes up quite a lot of space so we have moved it to a separate notebook. Go to [BiasVariance Notebook](BiasVariance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS Exercise if Time 5: Bias Variance\n",
    "Book Problem 2.24 part (a)\n",
    "\n",
    "Short Version:\n",
    "   \n",
    "  - The target function is $f(x) = x^2$ and the cost is Least Squares.\n",
    "\n",
    "  - Sample two points $x_1, x_2$ from $[-1, 1]$ uniformly at random to get the data set $D = \\{(x_1, x_1^2), (x_2, x_2^2)\\}$\n",
    "\n",
    "  - Use hypothesis set $\\{h(x) = ax +b\\mid a,b\\in\\Bbb R\\}$ i.e. lines. There are two parameters $a$ and $b$.\n",
    "\n",
    "  - Given a data set $D = \\{(x_1, x_1^2), (x_2, x_2^2)\\}$ the algorithm returns the line that fits these points.\n",
    "\n",
    "  - Your task is to write down an analytical expression for $\\bar{g} = \\mathbb{E}_D [h_D]$ where $h_D$ is the hypothesis learned on D.\n",
    "\n",
    "**Step 1.** What is the in sample error of $h_D$ and why?\n",
    "\n",
    "**Step 2.** Given $D$ what are $a, b$ (defined by the line between $(x_1, x_1^2)$ and  $(x_2, x_2^2)$)? Hint: $x_2^2- x_1^2 = (x_2-x_1)(x_2 + x_1)$.\n",
    "\n",
    "**Step 3.** What is the expected value of the slope $a$ over $x_1$ and $x_2$?\n",
    "\n",
    "**Step 4.** What is the expected value of the intercept $b$ over $x_1$ and $x_2$? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**More hints**\n",
    "For the uniform distribution over $[-1,1]$ the mean is $0$ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
