{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Transforms for Linear Models\n",
    "In this exercise you will experiment with nonlinear transforms for\n",
    "adding expressive power to our linear models. \n",
    "\n",
    "As in one of the examples in the lectures, the exercise considers adding polynomial features (such as $x_1^2$ or $x_1 x_2$ for a feature vector $(x_1,x_2)$).\n",
    "\n",
    "Download the file **code_nonlinear.py**, which is the file you are going to work with.\n",
    "If you want you are of course welcome to copy the code from this file into this notebook in any way you like.\n",
    "\n",
    "## Task 1: View the data\n",
    "Your first task is to have a look at the data. We have implemented plotting tools for your convenience. Visualize the four data sets we will be working with by running:\n",
    "\n",
    "**python3 code_nonlinear.py -plot**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Perceptron Algorithm \n",
    "We will use the perceptron learning algorithm. \n",
    "As you can see from your plots above, the data sets are not linearly separable as given.\n",
    "Since we may want to run the perceptron algorithm, you should ensure the algorithm runs only for a limited number of iterations and return the best solution found so far (The pocket algorithm).\n",
    "\n",
    "* Complete the class PerceptronClassifier (methods **fit**, **predict** and **score**)\n",
    "\n",
    "You can test it from the command line by calling **python3 code_nonlinear.py -platest**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Square Transform\n",
    "We denote a feature transform by $\\phi$. In the lectures we showed you a simple non linear transform that was able to separate a simple circular data set in feature space. In the lecture we used $\\phi(x_1,x_2) = (x_1^2 + x_2^2)$. Here you must instead implement the feature transform $\\phi(x_1,x_2) = (1,x_1^2,x_2^2)$.\n",
    "\n",
    "* Implement the square transform in the method **square_transform** \n",
    "\n",
    "you can test it with **python3 code_nonlinear.py -square**. We have added code that visualizes the decision boundary both in feature space and the original input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 4: General Polynomial Transform\n",
    "As you can tell, not all the data sets become linear separable with our current non-linear transform. \n",
    "Now you must to come up with a better non-linear transform that makes the data linearly separable in the transformed space. We suggest you use the general form of polynomials of degree 3, i.e.: $\\phi(x_1,x_2) = (x_1^i \\; x_2^j \\text{  for  } 0 \\leq i+j \\leq 3)$. This should give $\\binom{2+3}{3}=10$ features where one of them is the all ones vector. \n",
    "\n",
    "We will apply the transform to all the datasets, run the perceptron learning algorithm, and plot the results.\n",
    " All you need to do is to implement the fancy transform.\n",
    "\n",
    "* Complete the method **poly_transform** \n",
    "\n",
    "Test your entire algorithm on all data sets by running **python3 code_nonlinear.py -run**\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5:  Linear Regression for Classification\n",
    "\n",
    "Linear regression computes the linear model (line in 2D) that best approximates the real valued target function.\n",
    "It works by computing the vector\n",
    "\n",
    "$$\n",
    "w_{\\textrm{lin}} = \\textrm{argmin}_w \\sum_i (w^\\intercal x_i - y_i)^2.\n",
    "$$\n",
    "\n",
    "The labels can be any real numbers.\n",
    "In our setting (classification) the labels are either $+1$ or $-1$, which is a small subset of the real numbers.\n",
    "Linear regression finds the best linear fit to these values over the input points.\n",
    "Compare this to the perceptron linear algorithm, that simply finds *any* linear fit (without considering the distance from the points to the line).\n",
    "\n",
    "We can use exactly the same method to classify new points, i.e., we compute $\\textrm{sign}(w_\\textrm{lin}^\\intercal x)$.\n",
    "Notice that this is meaningful in the sense that linear regression tried to fit $+1$ and $-1$, so applying the sign function makes sense.\n",
    "\n",
    "**Complete the code in LinRegClassifier**. You may copy code from last week as you wish.\n",
    "\n",
    "\n",
    "Call **python3 code_nonlinear.py -linreg** to run the same tests as for the perceptron classifier and see the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## BONUS Task if Time 6: Understand the Plotting Code\n",
    "\n",
    "Take a look at the **plot_contour** code and see if you can figure out what it is doing to generate the plots of the non-linear decision boundaries. Only do this exercise after having completed all theory exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
