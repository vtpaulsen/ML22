{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Exercises Week 4\n",
    "**Like last week, it is very imporant that you try to solve every exercise. \n",
    "If you do not solve the exercise, focus on understanding the question, and try to figure out what it is you do not understand.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Convex Functions \n",
    "\n",
    "See https://en.wikipedia.org/wiki/Convex_function for the definitions of convex functions.\n",
    "At the lectures we saw two definitions. But there is a third one that is usually easier and is as follows.\n",
    "\n",
    "Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a function that is twice differentiable.\n",
    "\n",
    "Define the Hessian $H$ as the matrix of second order derivatives as (a matrix of shape $n \\times n$):\n",
    "$$H = \\left[ \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\right]_{i,j}$$\n",
    "\n",
    "A function f is convex if $H$ is a Positive Semidefinite Matrix (PSD) which is the case if $\\forall x\\in \\mathbb{R}^n$,  $ x^\\intercal H x \\geq 0  $ \n",
    "\n",
    "For a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ the Hessian is $H = f''$, and the condition says that $\\forall x: f''(x) \\geq 0$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Which of the following functions are convex on ${\\mathbb R}$? \n",
    "\n",
    "-   $f(x) = 2$\n",
    "\n",
    "-   $f(x) = -\\ln (x), x>0$\n",
    "\n",
    "-   $f(x) = x^3$\n",
    "\n",
    "-   $f(x) = x^2 + x^4$\n",
    "\n",
    "\n",
    "**hint: use the newly defined definition to determine convexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Lets plot them\n",
    "x = np.linspace(-1, 1, 1000)\n",
    "xp = x[x>0]\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x, [2 for y in x], 'r-', label='f(x)=2')\n",
    "plt.plot(xp, [-np.log(z) for z in xp], 'g-', label='f(x)=ln(x), x>0')\n",
    "plt.plot(x, x**3, 'b-', label='f(x)=x^3')\n",
    "plt.plot(x, x**2 + x**4, 'y-', label='f(x)=x^2+x^4')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Convexity of Linear Regression Cost Funtion\n",
    "In this exercise your job is to prove that the cost function for linear regression is convex.\n",
    "**Prove that the function $E_\\mathrm{in}(w) = \\frac{1}{n} \\|Xw -y\\|^2$ is convex.** \n",
    "\n",
    "\n",
    "\n",
    "- Hint 1: Prove that for all matrices $A$, it holds that $A^\\intercal A$ is Positive Semidefinite. \n",
    "- Hint 2: Compute the Hessian of $E_\\mathrm{in}(w)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Gradient Descent \n",
    "Let $f_a(x_1, x_2) = \\frac{1}{2}(x_1^2 + a\\cdot x_2^2)$\n",
    "\n",
    "Where $a$ is parameter that we will change.\n",
    "\n",
    "**Your task is to write a gradient descent algorithm that finds a minimizer of $f$, where we have decided that the starting point for the gradient descent is (256, 1). It should be possible for you to figure out what the local (global) minimum is, as well as the gradient.**\n",
    "- Test your algorithm by running the cell.\n",
    "- run your gradient descent algorithm for at least 40 steps to see if it converges. \n",
    "You must save the sequence of elements (2d points) considered in your gradient descent algorithm for visualization. \n",
    "We have added code to visualize this sequence.\n",
    "\n",
    "- Try a=1, 4, 16, 64, 128, 256 and adjust the step size to see if you can make it converge.\n",
    "    **hint - after trying different values for the stepsize also try approximately 1/a (for a > 1)**\n",
    "- What do you see? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(a, x):\n",
    "    return 0.5 * (x[0]**2 + a * x[1]**2)\n",
    "\n",
    "def visualize(a, path, ax=None):\n",
    "    \"\"\"\n",
    "    Make contour plot of f_a and plot the path on top of it\n",
    "    \"\"\"\n",
    "    y_range = 10\n",
    "    x = np.arange(-257, 257, 0.1)\n",
    "    y = np.arange(-y_range, y_range, 0.1)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    z = 0.5 * (xx**2 + a * yy**2)\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(16, 13))\n",
    "    h = ax.contourf(xx, yy, z, cmap=plt.get_cmap('jet'))\n",
    "    ax.plot([x[0] for x in path], [x[1] for x in path], 'w.--', markersize=4)\n",
    "    ax.plot([0], [0], 'rs', markersize=8) # optimal solution\n",
    "    ax.set_xlim([-257, 257])\n",
    "    ax.set_ylim([-y_range, y_range])\n",
    "\n",
    "def gd(a, step_size=0.1, steps=40):\n",
    "    \"\"\" Run Gradient descent\n",
    "        params:\n",
    "        a - the parameter that define the function f\n",
    "        step_size - constant stepsize to use for gradient descent\n",
    "        steps - number of steps to run\n",
    "        \n",
    "        Returns: out, list with the sequence of points considered during the descent.         \n",
    "    \"\"\"\n",
    "    out = []\n",
    "    x = np.array([256.0, 1.0]) # starting point\n",
    "\n",
    "    ### YOUR CODE HERE    \n",
    "    ### END CODE\n",
    "    return out\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 16))\n",
    "ateam = [[1, 4, 16], [64, 128, 256]]\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax = axes[i][j]\n",
    "        a = ateam[i][j]\n",
    "        path = gd(a, step_size=0.1, steps=40) # use good step size here instead of standard value\n",
    "        visualize(a, path, ax)\n",
    "        ax.set_title('Gradient Descent a={0}'.format(a), fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4:  Show that the cost function for Logistic Regression is convex\n",
    "### Try at least the first part. Do the second part after coding exercises if time.\n",
    "\n",
    "In class we derived the loss function for logistic regression based on negative log likelihood. The loss function was as follows:\n",
    "$$\n",
    "E_{in}(w) = - \\frac{1}{n}\\sum_{i=1}^n \\ln(\\sigma(y_i w^\\intercal x_i)) = \\frac{1}{n}\\sum_{i=1}^n \\ln(1 + e^{-y_i w^\\intercal x_i})\n",
    "$$\n",
    "where $\\sigma(z) = \\frac{1}{1+e^{-z}}$ is the sigmoid function with derivative $\\frac{\\partial \\sigma}{\\partial z} = \\sigma(z)\\sigma(-z)$.\n",
    "\n",
    "\n",
    "We need to prove that $E_{in}(w)$ is a convex function (data X, y fixed as usual).\n",
    "A sum of convex functions is convex so we can ignore the sum and focus on just one element. Similarly, the factor $1/n$ does not change convexity. Thus it suffices to show that\n",
    "$$\n",
    "f(w) = -\\ln(\\sigma(y w^\\intercal x))\n",
    "$$\n",
    "is a convex function (of $w$) for any fixed $x$ and $y$.\n",
    "\n",
    "\n",
    "\n",
    "We will do this in simple steps. First let us assume that x and w are 1D vectors i.e. numbers.\n",
    "To prove that $f$ is convex we can prove that $f''(w) \\geq 0$ for all $w$.\n",
    "* Step 1. Prove that $f'(w) = -\\sigma(-ywx)yx$.\n",
    "* Step 2. Prove that $f''(w) =  x^2 \\sigma(yw x)\\sigma (-yw x)$.\n",
    "* Step 3. Argue that $f''(w) \\geq 0$ for all w\n",
    "\n",
    "\n",
    "To generalize this to d-dimensional $w$ and $x$, we do the same steps, just with partial derivates. \n",
    "\n",
    "* Step 1. Show that $\\partial f/\\partial w_i = -\\sigma(-yw^\\intercal x)yx_i$.\n",
    "\n",
    "The Hessian matrix is the second order matrix of derivatives.\n",
    "\n",
    "* Step 2. Show that the Hessian of f is $\\sigma(yw^\\intercal x) \\sigma(-yw^\\intercal x) x x^\\intercal$ (note that this is an outer product) \n",
    "\n",
    "* Step 3. Show that $\\sigma(yw^\\intercal x) \\sigma(-yw^\\intercal x) x x^\\intercal$ is a Positive Semidefinite Matrix\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS Exercise if Time 5: Softmax Gradient\n",
    "As described in the softmax note, we define the softmax function as follows:\n",
    "$$\n",
    "\\textrm{softmax}:\\mathbb{R}^K \\rightarrow \\mathbb{R}^K, \\quad\n",
    "\\textrm{softmax}(x)_j =\n",
    "\\frac{e^{x_j}}\n",
    "{\\sum_{i=1}^K e^{x_i}}\\quad\n",
    "\\textrm{ for }\\quad j = 1, \\dots, K.\n",
    "$$\n",
    "where  $\\textrm{softmax}(x)_j$ denote the $j$'th output of the function\n",
    "\n",
    "\n",
    "Show that the matrix of derivatives of the softmax function is as follows.\n",
    "$$\n",
    "\\left[\\frac{\\partial \\textrm{softmax}}{\\partial x}\\right]_{i,j} =\n",
    "\\frac\n",
    "{\\partial \\;\\textrm{softmax}(x)_i}\n",
    "{\\partial x_j} =\n",
    "(\\delta_{i,j} - \\textrm{softmax}(x)_j)\n",
    "\\textrm{softmax}(x)_i\\quad\\quad\\text{where}\\quad\\quad\n",
    "\\delta_{ij}=\\begin{cases}1 &\\text{if }i=j\\\\\n",
    "0 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
