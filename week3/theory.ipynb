{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Exercises week 3\n",
    "**Like last week, it is very imporant that you try to solve every exercise. \n",
    "If you do not solve the exercise, focus on understanding the question, and try to figure out what it is you do not understand.**\n",
    "\n",
    "\n",
    "The TA's will be very happy to answer questions during the TA session or on the board.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feasibility of Learning\n",
    "\n",
    "\n",
    "## Exercise 1.12 from [LFD]:\n",
    "A friend comes to you with a learning problem. She says the target function is <i>completely</i> unknown, but she has 4000 data points. She is willing to pay you to solve her problem and produce for her a $g$ which approximates $f$. What is the best that you can promise her among the following:\n",
    "\n",
    "<b>a</b> After learning you will provide her with a $g$ that you will guarantee approximates $f$ well out of sample.\n",
    "\n",
    "<b>b</b> After learning you will provide her with a $g$, and with high probability the $g$ which you produce will approximate $f$ well out of sample.\n",
    "\n",
    "<b>c</b> One of two things will happen.\n",
    "- <b>(i)</b> You will produce a hypothesis $g$;\n",
    "- <b>(ii)</b> You will declare that you failed.\n",
    "\n",
    "If you do return a hypothesis $g$, then with high probability the $g$ which you produce will approximate $f$ well out of sample.\n",
    "\n",
    "\n",
    "<br>Solution:<br/> The solution is answer c. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. In-Sample and Out-of-Sample Error\n",
    "\n",
    "Consider an input domain $X=\\{x_1,x_2,x_3,x_4\\}$. Let the unknown target function $f$ assign labels $1$ to all $4$ elements in $X$.\n",
    "\n",
    "Consider a hypothesis $h$ such that $h(x_1)=h(x_2)=h(x_3)=1$ and $h(x_4)=-1$.\n",
    "\n",
    "<b>Question 1</b>: What is the worst possible $E_{in}(h)$ over any data set $S$ of $5$ samples of the form $(x,f(x))$ where $x \\in X$?\n",
    "\n",
    "<b>Solution</b>: Here samples are in form of $x_1, x_2, \\ldots$. Input domain: $x_1, x_2. .. $\n",
    "\n",
    "\n",
    "\n",
    "<b>Question 2</b>: What is the best possible $E_{in}(h)$ over any data set $S$ of $5$ samples of the form $(x,f(x))$ where $x \\in X$?\n",
    "\n",
    "<b>Question 3</b>: Consider an unknown distribution $D$, such that $D(x_1)=D(x_2)=D(x_3)=1/6$ and $D(x_4)=1/2$ (here $D(x_i)$ denotes the probability of seeing the sample $x_i$). What is $E_{out}(h)$ under this distribution? What is the worst possible $E_{in}(h)$ we could get if we sample $5$ elements from $D$? What is the best $E_{in}(h)$?\n",
    "\n",
    "**Solution:** Sandsynligheden for at vi tager fejl, 1/2. \n",
    "\n",
    "<b>Question 4</b>: Give an example of a distribution $D$ over $X$ such that $E_{out}(h)=1$.\n",
    "\n",
    "<b>Question 5</b>: Give an example of a distribution $D$ over $X$ such that $E_{out}(h)=0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generalization\n",
    "\n",
    "\n",
    "## Questions:\n",
    "The Hoeffding bound gives us the following guarantee:\n",
    "$$\n",
    "\\Pr[|E_{in}-E_{out}| > \\varepsilon] \\leq 2Me^{-2\\varepsilon^2 n},\n",
    "$$\n",
    "where the probability is over the random choice of the sample.\n",
    "\n",
    "<b>Question 1: </b> \n",
    "Does the Hoeffding bound give any meaningful bounds on $E_{in}$ and $E_{out}$ for the perceptron learning model?  \n",
    "\n",
    "**Solution:** Nej, Perceptron har et uendelig sample-space, så hvis vi sætter M = uendelig .. \n",
    "\n",
    "<b>Question 2: </b> \n",
    "Assume you use a hypothesis set $H$ with $|H|=M=10^6$. How many samples $n$ do you need to guarantee that $|E_{in} - E_{out}| < 0.1$ with probability at least $0.95$? \n",
    "\n",
    "<b>Solution: </b>\n",
    "Use slide 14 from UnionBound.pdf. \n",
    "\n",
    "$$\n",
    "    \\text{We have } \\epsilon = 0.1 \\text{ and } M = 10^6\\\\\\\\\n",
    "    2 \\cdot 10^6 \\cdot e^{-2 \\cdot 0.1^2 \\cdot n} < 0.05\\\\\\\\\n",
    "    \\Pr[|E_{in}-E_{out}| > \\varepsilon] \\text{ er sandsynligheden for at vi fejler.}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<b>Question 3: </b> \n",
    "Assume you use a hypothesis set $H$ with $|H|=M=10^6$ and you have $n=10^4$ samples. How small can you guarantee that $|E_{in} - E_{out}|$ is with probability at least $0.95$? \n",
    "\n",
    "**Solution** Find episilon\n",
    "\n",
    "\n",
    "<b>Question 4: </b> \n",
    "Assume you have $n=10^4$ samples and would like to guarantee that $|E_{in} - E_{out}|<0.2$ with probability at least $0.95$. How large a hypothesis set can you use?\n",
    "\n",
    "**Solution** Find M\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Noisy Targets\n",
    "In the lectures, we extended the supervised learning setup to a case where the labels are noisy. Formally, for any feature vector $x$, there is a distribution $P(y \\mid x)$ on the label $y$.\n",
    "\n",
    "Assume the data distribution $D$ gives a uniform random feature vector among a fixed set of three vectors $x_1,x_2$ and $x_3$.\n",
    "\n",
    "Assume that $Pr(y=1 \\mid x_1) = 2/3$ (and thus $Pr(y=-1 \\mid x_1) = 1/3$). Also, assume $Pr(y=1 \\mid x_2) = 1/2$ and $Pr(y=1 \\mid x_3) = 1/4$.\n",
    "\n",
    "<b>Question</b>: What is the best possible out-of-sample error that any hypothesis $h : \\{x_1,x_2,x_3\\} \\to \\{-1,1\\}$ can achieve? And what are the predictions made by that hypothesis $h$?\n",
    "\n",
    "<b>Question (*A bit hard, so maybe skip)</b>: Does it help to allow a <i>randomized</i> hypothesis? A randomized hypothesis $h$ is one such that on any given $x$, we randomly output $1$ with probability $p_{h,x}$ and $-1$ with probability $1-p_{h,x}$. Here the probability $p_{h,x}$ thus depends on both $h$ and $x$. \n",
    "\n",
    "Hint: Try to look at an example where $Pr(y=1 \\mid x)=p$ for a $p \\geq 1/2$ and assume a randomized hypothesis $h$ predicts $1$ with probability $p_{h,x}$. Then determine the best choice of $p_{h,x}$ to minimize the chance of mispredicting the label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Linear Regression and the missing inverse\n",
    "In linear regression, given data matrix $X$ and labels vector $y$ the optimal weight vector $w$ (minimizing $\\|Xw-y\\|_2^2$), is found simply by computing the matrix product\n",
    "$$\n",
    "(X^\\intercal X)^{-1}X^\\intercal y\n",
    "$$\n",
    "That only makes sense if $(X^\\intercal X)$ is in fact invertible.\n",
    "\n",
    "In the lectures I suggested that if indeed $(X^\\intercal X)$ is not invertible then we should remove linear dependent columns from $X$.\n",
    "In this exercise you must argue that this is a good idea.\n",
    "\n",
    "To do this, you must prove/argue the two following things\n",
    "* If $(X^\\intercal X)$ is not invertible then $X$ contains linearly dependent columns\n",
    "* Removing linear dependent columns from X does not change the cost of an optimal solution $w$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HINT 1: you can use a well known linear algebra fact that rank(X) = rank($X^\\intercal X$) = rank($X X^\\intercal$)\n",
    "\n",
    "HINT 2: $Xw$ is in the column space of $X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Matrix of Derivatives\n",
    "\n",
    "In Linear Regression we define the in sample error as (ignoring the normalizing factor 1/n)\n",
    "\n",
    "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 $$ \n",
    "\n",
    "Let $X$ be the the data matrix of shape $n \\times d$ with data point $x_i$ as the $i$'th row. Let $y$ be the label vector of shape $n \\times 1$ with label $y_i$ as the $i$'th entry. Let $w$ be the weight vector of shape $d \\times 1$.  \n",
    "\n",
    "$$X=\\begin{pmatrix}\n",
    "- & x_1^T & - \\\\\n",
    "- & \\vdots & - \\\\\n",
    "- & x_n^T & - \\\\\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times d}\\quad\\quad\\quad\n",
    "y=\\begin{pmatrix}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times 1}$$\n",
    "\n",
    "The in-sample error rate $E_{in}$ is then equal to \n",
    "\n",
    "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 = \\|Xw-y\\|^2 = (Xw-y)^\\intercal (Xw-y)$$\n",
    "\n",
    "\n",
    "In the lecture we proved that for Linear Regression the optimal weight vector $w_\\textrm{lin}$ for minimizing $E_\\textrm{in}$ was $w_\\textrm{lin}=(X^\\intercal X)^{-1} X^\\intercal y$. \n",
    "\n",
    "To do this we used facts about the derivatives. \n",
    "* If we have a function $f(z): \\mathbb{R}^{a} \\rightarrow \\mathbb{R}^b$ such that $f(z) = [f_1(z),\\dots, f_b(z)]$ then the matrix of derivatives $\\frac{\\partial f}{\\partial z}$ is of size $b\\times a$ where\n",
    "$$ \\left[\\frac{\\partial f}{\\partial z} \\right]_{i,j} = \\frac{\\partial f_i}{\\partial z_j} $$\n",
    "\n",
    "For example: Let $f(x): R^2 \\rightarrow R^3$ be the function $f([x_1, x_2]) = [x_1, x_2, x_1 \\cdot x_2]$ then \n",
    "the matrix of derivatives has shape $3 \\times 2$ and looks like $ \\frac{\\partial f}{\\partial x} =\n",
    " \\begin{bmatrix}\n",
    "  1 & 0 \\\\\n",
    "  0 & 1  \\\\\n",
    "  x_2  & x_1 \\\\\n",
    " \\end{bmatrix} $\n",
    "\n",
    "In our proof we used the following identities about the matrix of derivatives\n",
    "\n",
    "* $f: R^d \\rightarrow R^n, f(z) = Xz-y$, the matrix of derivatives $\\frac{\\partial f}{\\partial z}$ is $X$\n",
    "* $g: R^d \\rightarrow R, g(z) = z^\\intercal z$, the matrix of derivatives $\\frac{\\partial g}{\\partial z}$ is $2z^\\intercal$\n",
    "\n",
    "\n",
    "## Your job is to prove the two identities. \n",
    "\n",
    "* Let $f(z) = Xz - y$, Where $X$ is a $n \\times d$ matrix, $y$ is a $n\\times 1$ vector and $z$ is a $d \\times 1$ vector (function from $\\mathbb{R}^d \\rightarrow \\mathbb{R}^n$). \n",
    "Show that the matrix of derivatives of $f$ is X. \n",
    "\n",
    "Hint: You can think of $f = [f_1,\\dots,f_n]$ as n output functions where $f_i(z) = x_{i}^\\intercal z - y_i$ and $x_i$ is the i'th row of $X$ (as a column vector) and $y_i$ is the i'th entry in vector $y$. Start with $\\frac{\\partial f_1}{\\partial z_1}$ to see if a pattern emerges\n",
    "* Let $g(z) = z^\\intercal z$ where z is a vector (the squared norm of $z$). Show that the matrix of derivatives is $2z^\\intercal$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Implementing Linear Regression\n",
    "\n",
    "In this exercise your task is to implement Linear Regression.\n",
    "\n",
    "See **linear_regression.py** for starter code.\n",
    "\n",
    "**You need to complete the LinearRegressor class by completing the following methods**\n",
    "- implement hardcode_bias \n",
    "- implement predict \n",
    "- implement score\n",
    "- implement fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
